{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOrg5ysfB5F1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dense, Input, Reshape, Conv2DTranspose, Flatten\n",
        "from keras import backend as k\n",
        "from keras.constraints import Constraint\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdMd0b0HCrbM"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    (X, _),(_, _) = keras.datasets.mnist.load_data()\n",
        "    X = X[:6000]\n",
        "    X = np.expand_dims(X,axis=-1)\n",
        "    X = X.astype('float')\n",
        "    X = (X-127.5)/127.5\n",
        "    return X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HCbOXHciIKB"
      },
      "outputs": [],
      "source": [
        "def get_real_samples(dataset, n_samples):\n",
        "    idx = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "    X = dataset[idx]\n",
        "    y = - np.ones((n_samples,1))\n",
        "    return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2Ll3ADcjFXu"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    X = np.random.randn(latent_dim * n_samples).reshape((n_samples, latent_dim))\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VBQuKUoimfY"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    latent_points = generate_latent_points(latent_dim, n_samples)\n",
        "    X = generator.predict(latent_points)\n",
        "    y = np.ones((n_samples,1))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtVS-wA_DZv5"
      },
      "outputs": [],
      "source": [
        "class ClipConstraint(Constraint):\n",
        "    def __init__(self, clip_value):\n",
        "        self.clip_value = clip_value\n",
        "\n",
        "    def __call__(self, weights):\n",
        "        return k.clip(weights, -self.clip_value, self.clip_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soWHmggHh0uG"
      },
      "outputs": [],
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return k.mean(y_true * y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpnKQgZZDxgR"
      },
      "outputs": [],
      "source": [
        "def build_generator(latent_dim : int):\n",
        "    init = RandomNormal(stddev = 0.02)\n",
        "\n",
        "    inputs = Input(shape = (latent_dim,))\n",
        "\n",
        "    dense_0 = Dense(128*7*7, kernel_initializer=init) (inputs)\n",
        "    relu_0 = LeakyReLU(alpha=0.2) (dense_0)\n",
        "    reshape_0 = Reshape((7,7,128)) (relu_0)\n",
        "\n",
        "    convt_0 = Conv2DTranspose(128, (4,4), (2,2), padding='same', kernel_initializer=init) (reshape_0)\n",
        "    norm_0 = BatchNormalization() (convt_0)\n",
        "    relu_1 = LeakyReLU(alpha=0.2)(norm_0)\n",
        "\n",
        "    convt_1 = Conv2DTranspose(128, (4,4), (2,2), padding='same', kernel_initializer=init) (relu_1)\n",
        "    norm_1 = BatchNormalization() (convt_1)\n",
        "    relu_2 = LeakyReLU(alpha=0.2)(norm_1)\n",
        "\n",
        "    conv_0 = Conv2D(1, (7,7), activation ='tanh', padding='same', kernel_initializer=init) (relu_2)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=conv_0, name='mnist_generator')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzyFThK1I6Nt"
      },
      "outputs": [],
      "source": [
        "def build_critic(image_shape = (28,28,1)):\n",
        "    init = RandomNormal(0.02)\n",
        "    const=ClipConstraint(0.01)\n",
        "\n",
        "    inputs = Input(shape=image_shape)\n",
        "\n",
        "    conv_0 = Conv2D(64,(4,4), (2,2), padding='same', kernel_initializer=init, kernel_constraint=const)(inputs)\n",
        "    norm_0 = BatchNormalization()(conv_0)\n",
        "    relu_0 = LeakyReLU(alpha=0.2)(norm_0)\n",
        "\n",
        "    conv_1 = Conv2D(64,(4,4), (2,2), padding='same', kernel_initializer=init, kernel_constraint=const)(relu_0)\n",
        "    norm_1 = BatchNormalization()(conv_1)\n",
        "    relu_1 = LeakyReLU(alpha=0.2)(norm_1)\n",
        "\n",
        "    flatten = Flatten()(relu_1)\n",
        "    dense = Dense(1)(flatten)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=dense, name='critic')\n",
        "    model.compile(loss=wasserstein_loss, optimizer=RMSprop(learning_rate=0.0004))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dak-m1ZFhGKF"
      },
      "outputs": [],
      "source": [
        "def build_gan(generator, critic):\n",
        "    for layer in critic.layers:\n",
        "        if not isinstance(layer, BatchNormalization):\n",
        "            layer.trainable=False\n",
        "\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            generator,\n",
        "            critic\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(loss=wasserstein_loss, optimizer=RMSprop(learning_rate=0.0005))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsJP1Q5dh9qy"
      },
      "outputs": [],
      "source": [
        "def show_performance(epoch, generator, latent_dim, n_samples = 36):\n",
        "    X, _ = generate_fake_samples(generator, latent_dim, n_samples)\n",
        "    X = (X+1)/2.0\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        plt.subplot(10,10,i+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(X[i,:,:,0], cmap = 'gray_r')\n",
        "    filename = f'generated_epoch_{epoch}.png'\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "    model_name = f'generator_{epoch}.h5'\n",
        "    generator.save(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpUtW4YQlY9C"
      },
      "outputs": [],
      "source": [
        "def plot_history(critic_loss_real, critic_loss_fake, gan_loss):\n",
        "    plt.plot(critic_loss_real, label='Discriminator Real')\n",
        "    plt.plot(critic_loss_fake, label='Discriminator Fake')\n",
        "    plt.plot(gan_loss, label='Generator')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('GAN Training History')\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('plot_line_plot_loss.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dC_IYRWmHGZ"
      },
      "outputs": [],
      "source": [
        "def train(generator, critic, gan, dataset, latent_dim, epochs, batch_size, n_critics):\n",
        "    batch_per_epoch = int(dataset.shape[0]/batch_size)\n",
        "    n_steps = batch_per_epoch * epochs\n",
        "    half_batch = batch_size // 2\n",
        "    critic_loss_real, critic_loss_fake, gan_loss_hist = [],[],[]\n",
        "    epoch = 0\n",
        "\n",
        "\n",
        "    for step in range(n_steps):\n",
        "        critic_temp_real, critic_temp_fake = [], []\n",
        "        for _ in range(n_critics):\n",
        "            X_real, y_real = get_real_samples(dataset, batch_size)\n",
        "            c1 = critic.train_on_batch(X_real, y_real)\n",
        "            critic_temp_real.append(c1)\n",
        "\n",
        "            X_fake, y_fake = generate_fake_samples(generator, latent_dim, batch_size)\n",
        "            c2 = critic.train_on_batch(X_fake, y_fake)\n",
        "            critic_temp_fake.append(c2)\n",
        "\n",
        "        critic_loss_real.append(np.mean(c1))\n",
        "        critic_loss_fake.append(np.mean(c2))\n",
        "        X_fake = generate_latent_points(latent_dim, batch_size)\n",
        "        y_fake = -np.ones((batch_size,1))\n",
        "\n",
        "        gan_loss = gan.train_on_batch(X_fake, y_fake)\n",
        "        gan_loss_hist.append(gan_loss)\n",
        "\n",
        "        if (step+1) % batch_per_epoch == 0:\n",
        "            show_performance(epoch, generator, latent_dim, batch_size)\n",
        "            plot_history(critic_loss_real, critic_loss_fake, gan_loss)\n",
        "            epoch+=1\n",
        "            print(f'\\nstarting epoch {epoch}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlIkfDclCSoH"
      },
      "outputs": [],
      "source": [
        "LATENT_DIM = 50\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "N_CRITICS = 5\n",
        "\n",
        "critic = build_critic()\n",
        "generator = build_generator(LATENT_DIM)\n",
        "gan = build_gan(generator, critic)\n",
        "\n",
        "dataset = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOhCF85MAVKA"
      },
      "outputs": [],
      "source": [
        "plot_model(critic, 'critic.png', show_shapes = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCcTvexuAOqp"
      },
      "outputs": [],
      "source": [
        "plot_model(generator, 'generator.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bx7SxJ3gMM5J"
      },
      "outputs": [],
      "source": [
        "train(generator, critic, gan, dataset, LATENT_DIM, 50, BATCH_SIZE, N_CRITICS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}