{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLV90yG2NjWG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import (\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    LeakyReLU,\n",
        "    Conv2DTranspose,\n",
        "    BatchNormalization,\n",
        "    Reshape,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Input,\n",
        ")\n",
        "from keras.utils import plot_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets.mnist import load_data\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LATENT_DIM = 128\n",
        "IMAGE_SHAPE = (28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT3vupXqNnH9"
      },
      "outputs": [],
      "source": [
        "def load_mnist_data():\n",
        "    (X_train, _), (_, _) = load_data()\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "    X = X_train.astype(\"float32\") / 255.0\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqOvvO7Eb4eV"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(image_size=(28, 28, 1)):\n",
        "    inputs = Input(shape=image_size)\n",
        "\n",
        "    conv_1 = Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\")(inputs)\n",
        "    relu_1 = LeakyReLU(0.2)(conv_1)\n",
        "    conv_2 = Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\")(relu_1)\n",
        "    relu_2 = LeakyReLU(0.2)(conv_2)\n",
        "    flatten_1 = Flatten()(relu_2)\n",
        "    dense_1 = Dense(1, activation=\"sigmoid\")(flatten_1)\n",
        "\n",
        "    discriminator = Model(inputs=inputs, outputs=dense_1, name=\"discriminator\")\n",
        "    discriminator.compile(\n",
        "        metrics=[\"accuracy\"],\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(0.002, beta_1=0.5),\n",
        "    )\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY0_x8d9Ns2z"
      },
      "outputs": [],
      "source": [
        "discriminator = build_discriminator()\n",
        "plot_model(discriminator, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynoNh3ntOTHl"
      },
      "outputs": [],
      "source": [
        "def build_generator(latent_dim: int):\n",
        "    input_noise = Input(shape=(latent_dim,))\n",
        "\n",
        "    dense_1 = Dense(7 * 7 * 128)(input_noise)\n",
        "    relut_0 = LeakyReLU(0.2)(dense_1)\n",
        "    bn_1 = BatchNormalization(momentum=0.8)(relut_0)\n",
        "    reshape_1 = Reshape((7, 7, 128))(bn_1)\n",
        "\n",
        "    convt_1 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\")(reshape_1)\n",
        "    relut_1 = LeakyReLU(0.2)(convt_1)\n",
        "    bn_2 = BatchNormalization(momentum=0.8)(relut_1)\n",
        "\n",
        "    convt_2 = Conv2DTranspose(512, (4, 4), strides=(2, 2), padding=\"same\")(bn_2)\n",
        "    relut_2 = LeakyReLU(0.2)(convt_2)\n",
        "    bn_3 = BatchNormalization(momentum=0.8)(relut_2)\n",
        "\n",
        "    conv_1 = Conv2D(1, (3, 3), padding=\"same\", activation=\"sigmoid\")(bn_3)\n",
        "\n",
        "    generator = Model(inputs=input_noise, outputs=conv_1)\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3aLHMUvQjFq"
      },
      "outputs": [],
      "source": [
        "generator = build_generator(128)\n",
        "plot_model(generator, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWWz1u0ClAmI"
      },
      "outputs": [],
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    model = keras.Sequential()\n",
        "    model.add(generator)\n",
        "    model.add(discriminator)\n",
        "    opt = keras.optimizers.Adam(learning_rate=0.002, beta_1=0.5)\n",
        "    model.compile(optimizer=opt, loss=\"binary_crossentropy\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjWCLqJVf5zZ"
      },
      "outputs": [],
      "source": [
        "def get_real_samples(data: int, n_samples: int):\n",
        "    idx = np.random.randint(0, len(data), n_samples)\n",
        "    X = data[idx]\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voSF3Cztfy0U"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim: int, n_samples: int):\n",
        "    latent_vectors = np.random.randn(latent_dim * n_samples).reshape(\n",
        "        (n_samples, latent_dim)\n",
        "    )\n",
        "    return latent_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3dy7hltf3nP"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(generator, latent_dim: int, n_samples: int):\n",
        "    noise = generate_latent_points(latent_dim, n_samples)\n",
        "    X = generator.predict(noise)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiMO_WhwkB9q"
      },
      "outputs": [],
      "source": [
        "def save_plot(X, epoch: int, n=5):\n",
        "    for i in range(n * n):\n",
        "        plt.subplot(n, n, i + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(X[i, :, :, 0], cmap=\"gray_r\")\n",
        "\n",
        "    filename = f\"gen_epoch_{epoch}.png\"\n",
        "    plt.savefig(filename)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0fqhtFMgNpf"
      },
      "outputs": [],
      "source": [
        "def show_images(latent_dim: int, epoch: int):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.rand(r * c, latent_dim)\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    fig, ax = plt.subplots(r, c)\n",
        "    count = 0\n",
        "\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            ax[i, j].imshow(generated_images[count, :, :, 0], cmap=\"gray_r\")\n",
        "            ax[i, j].axis(\"off\")\n",
        "    fig.savefig(f\"epoch_{epoch}_result.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5QWQ9e4izWt"
      },
      "outputs": [],
      "source": [
        "def summarize_performance(\n",
        "    epoch, generator, discriminator, data, latent_dim, sample_size=100\n",
        "):\n",
        "    X_real = get_real_samples(data, sample_size)\n",
        "    y_real = np.ones((sample_size, 1))\n",
        "    X_fake = generate_fake_samples(generator, latent_dim, sample_size)\n",
        "    y_fake = np.zeros((sample_size, 1))\n",
        "    _, acc_real = discriminator.evaluate(X_real, y_real, verbose=0)\n",
        "    _, acc_fake = discriminator.evaluate(X_fake, y_fake, verbose=0)\n",
        "\n",
        "    print((acc_real * 100, acc_fake * 100))\n",
        "    save_plot(X_fake, epoch)\n",
        "    filename = \"generator_model_%03d.h5\" % (epoch + 1)\n",
        "    generator.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYef_gJfQqH_"
      },
      "outputs": [],
      "source": [
        "def train(generator, discriminator, gan, data, latent_dim, epochs=250, batch_size=128):\n",
        "    batch_per_epoch = int(data.shape[0] / batch_size)\n",
        "    half_batch = batch_size // 2\n",
        "\n",
        "    y_real = np.ones((half_batch, 1))\n",
        "    y_fake = np.zeros((half_batch, 1))\n",
        "\n",
        "    for i in range(epochs):\n",
        "        for j in range(batch_per_epoch):\n",
        "            X_real = get_real_samples(data, half_batch)\n",
        "            X_fake = generate_fake_samples(generator, latent_dim, half_batch)\n",
        "            print(X_real.shape, X_fake.shape)\n",
        "\n",
        "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
        "            d_loss, _ = discriminator.train_on_batch(X, y)\n",
        "\n",
        "            X_gan = generate_latent_points(latent_dim, batch_size)\n",
        "            y_gan = np.ones((batch_size, 1))\n",
        "\n",
        "            g_loss = gan.train_on_batch(X_gan, y_gan)\n",
        "\n",
        "            print(i + 1, j + 1, batch_per_epoch, d_loss, g_loss)\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            summarize_performance(i, generator, discriminator, data, latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBteLemAlq3P"
      },
      "outputs": [],
      "source": [
        "gan = build_gan(generator, discriminator)\n",
        "data = load_mnist_data()\n",
        "train(generator, discriminator, gan, data, latent_dim=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D5PPwabaoRq"
      },
      "outputs": [],
      "source": [
        "train(generator, discriminator, gan, data, latent_dim=128)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
